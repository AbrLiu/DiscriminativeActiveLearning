---
layout: post
title: Batch Active Learning
---
## Making Active Learning Practical
In our [last post][previous post], we learned about the active learning framework. In our framework, there is a lrage pool of easily gathered unlabeled data, and our learner helps reduce the labeling costs by asking for labels one at a time from the unlabeled pool. We saw a few possible query strategies that the learner can use, and that they can reduce the labeling costs significantly.

This is all very nice, but who has time to sit and label examples one at a time, when we have to wait for the learner to train a neural network before every single label request? Furthermore, we usually want to have a large batch of examples to label, so we can send them to many different annotators who can label them in parallel... So to make the ideas from the last post applicable to modern machine learning tasks, we need to slightly tweak our framework.

In each iteration, the learner will train a model on the labeled examples like before, only now he will **query a batch of examples**. The size of this batch is a free parameter of our framework, and depends on the way we do the labeling. Overall this seems like a rather meaningless change to our framework, but it adds some complexity.

### The Effects of the Batch Size
There are a few effects that the batch size has on our learning process. On the plus side, we get to label examples in parallel and suffer less of a delay to our process which stems from waiting for our neural networks to train. But this plus side has a cost.

On the down side, the learner has to select examples with less information. If a learner has to query a batch of size 100, then the 100th example he has to choose is chosen without seeing the labels to the first 99 examples. If we had those labels, the learner might have chosen a different 100th example which would have been better for our overall accuracy in the long run.

But there is an even worse down side to the batch setting. The learner can ask for a batch of 100 examples such that each of them is very informative, **but the information they give us is too similar**. Intuitively, if the learner has trouble confidently classifying the digit "0", it should be enough to get a few relevant examples of difficult "0"s for it to improve. But if the learner asks for 100 examples of "0"s, it isn't using it's batch budget very well...

This sort of problem can lead to a greedy strategy which chooses the top-K examples according to a standard measure **being worse than random sampling**. So intuitively, a good query strategy for the batch setting should not only chooses informative examples in the batch, but also choose examples that have small mutual information.

## Query Strategies For the Batch Setting
There were many papers published in recent years about active learning for neural networks, and we probably missed a few of them. Still, we'll try to detail the most notable modern methods, and for those we also provide implementations and code to recreate our experiments.

### Greedy Query Strategies
These strategies are adaptations for neural networks of classic query strategies which only query one label at a time. They do not take into account the mutual information between examples in a single batch, but rather simply **choose the top-K examples under the relevant score**.

#### Uncertainty-Based Strategy
We saw in the previous post that uncertainty sampling performs really well in the classical setting, and in general it is the most widely used query strategy due to it's effectiveness and simplicity. It isn't surprising then that this query strategy has been adapted to neural networks.
The most straightforward adaptation of uncertainty sampling is to simply treat the softmax scores of the neural network as a probability distribution over the labels \\(\hat{P}(y|x)\\), and use the regular decision rules. This approach is reasonable, but it is questionable how much the softmax scores can be interpreted as probabilities... They look like probabilities, but the training process of the neural network just tries to get the softmax scores of the training examples to be as close to one-hot as possible, and because the model is very powerful it tends to fit the data very well and so gets over confident about examples in general.

To deal with this issue, many people are working on ways of making neural networks output results which more accurately represent their confidence on a given input. The most well known work, and one which has been adapted to the active learning setting, is [Yarin Gal's work on Bayesian Neural Networks][Yarin Gal Paper]. In this work, the authors show that we can get an approximation of the posterior probability over the labels by running the input many times through the network **with dropout turned on** and then average the resulting softmax scores. This method can be easilly implemented for most modern neural network architectures, since they already use dropout for regularization. The resulting probability distribution over the labels after \\(T\\) iterations is then simply:

$$ P(y|x) \approx \frac{1}{T}\sum_{t=1}^{T}\hat{P}(y|x,drop_t) $$

This new confidence estimation can now be plugged into any of the possible uncertainty decision rules in order to choose the top-K examples. In their paper the authors compare many possible decision rules, and the best two decision rules were the two presented in the previous post for uncertainty sampling.

#### Margin-Based Strategy
In the previous post we saw that for linear models there is an equivalence between choosing examples which have a high uncertainty and choosing examples which are closest to the decision boundary. This is due to the fact that the probability of an example is decided by it's distance (or inner product) from the weight vector which is orthogonal to the decision boundary.

In neural networks though, this is hardly the case. The uncertainty score in neural networks is calculated with respect to the final representation layer. This means that choosing according to the uncertainty score is equivalent to choosing examples whose **representation** is closest to the decision boundary. The thing is, because the network is deep and complex, small changes to the input image can cause wild changes to the representation of the image. This is best seen in [adversarial examples][adversarial example paper].

Adversarial examples are a fascinating and quirky property of neural networks which stems from a very simple fact - in the same way we can calculate the derivative of the loss with respect to the weights of the network, we can calculate the derivative **with respect to the input**. So changing the weights in the opposite direction of the gradient, we can change the input image in the direction of the gradient and get a new image which slightly increases the loss. Doing this a few times can result in an image which is completely misclassified while looking basically the same as the original image. This property of neural networks and it's implications has led to a thriving field of neural network security research.

So coming back to active learning, we would like to be able to find the images which are closest to the decision boundary of our network, but the distance of the representation of the inputs to the softmax decision boundary is a bad proxy for that. The actual distance to the decision boundary, like many things in the world of deep learning, is intractable. So what can we do?

The cool solution [Frédéric Precioso's group came up with][adversarial AL paper], is to use adversarial examples to get a better proxy for the distance to the decision boundary. What they do is take each image in the unlabeled set, get it's classification according to the network and then find an adversarial example for it, such that the network makes a wrong classification on that image. They then calculate the distance between the original image and the adversarial example, and choose the top-K examples with the smallest distance to their adversarial example. Basically, they cross the decision boundary and check how far they had to go to cross it.

This doesn't mean that the adversarial example is the closest possible to the decision boundary, but it does give a reasonable heuristic upper bound on the distance to the decision boundary, which their method attempts to minimize.

### Batch-Aware Query Strategy - Core Set

This strategy takes the opposite approach to the previous ones. Instead of ignoring the batch mode and focusing on greedily choosing the examples that maximize some score, this method ignores the approach of maximizing some score, and instead tries to find a batch that is as diverse as possible, and represents the complete data distribution. But how can we find a batch which represents the complete unlabeled set as much as possible?

The solution, proposed independently by [Ran El-Yaniv's group][Ran Core Set] and [Silvio Savarese's group][Silvio Core Set] (with more impressive empirical results in Silvio's paper), draws it's inspiration from the concept of **core sets**. A core set is a subset of a dataset, such that when a model is trained on the subset it will produce a function that is close to the trained model on the entire dataset under some metric. This concept seems to be exactly what we want - active learning is basically a core set selection problem. If we were able to query a batch that is a core set of the unlabeled set, we would quickly be able to gather labeled data such that the resulting model will be as good as the model trained on the entire distribution.

In Silvio's paper, the authors show that under some assumptions on the model and the loss function, we can minimize a (not very tight) upper bound on the generalization error of the model by choosing a batch which covers the unlabeled set as much as possible, in an \\(\epsilon\\)-cover sense. This means that we should choose a batch such that when added to the labeled set, **the maximum distance between an unlabeled example and a labeled example is minimized**.

If this sounds like a hard thing to do, it's because it is. Still, there is a simple greedy algorithm which guarantees a 2-approximate solution for this problem - greedily choose the example whose distance from the closest labeled example is farthest until you have a full batch. This algorithm is fast to run and results in a batch which is guaranteed to be diverse, and cover the unlabeled set relatively well.

Still, the fun doesn't end here because while solving the original problem is NP-hard, that doesn't mean we can't try to improve on the approximate solution. The authors formulate the problem as a mixed integer optimization problem which tries to find solutions which minimize the maximum distance, and give the approximate solution as an initial starting point for the MIP solver. Even better, they formulate the MIP such that it is robust to outliers - there is a constant amount of examples which are allowed to have a distance larger than the maximum distance. This addition is important because the original greedy algorithm is susceptible to outliers, because it keeps choosing the examples which are farthest away.

There are no guarantees that we will be able to improve on the greedy solution in a reasonable amount of time using the above formulation, but the relaxation of allowing outliers usually leads to a small improvement (and makes the chosen batch contain less outliers).

There is only one thing we need to sort out - the distances between images in pixel space has little to do with the semantic distance between them, so just using the euclidean distance between the images isn't the best idea. What this method does is **use the image representation learned by the neural network that was trained on the labeled set as the space in which to calculate the distances between images**. So we simply need to train the network on the labeled set, get the network's embedding of the entire dataset and then run the core set method on that embedding.

#### Flexibility of the Core Set Approach
This approach has a unique advantage which isn't mentioned but is very relevant for practical purposes. So far we have talked about active learning only in the setting of classification problems, but that doesn't mean that we wouldn't want to use it on other problems (regression problems, structured learning, segmentation and so on).

All of the methods we've talked about besides core set assume we have access to a learned \\(\hat{P}(y|x)\\), but when our problem isn't a classification problem, this isn't true. While we could try to adapt the other methods to new scenarios, the core set approach simply assumes access to a learned representation of the input data, which is something that exists for a larger variety of scenarios (most ones involving neural networks). So this method automatically extends to many types of machine learning problems, which is a real plus. 

We can also think about extending this method to working on any type of data and problem **by having the learned representation be an embedding learned by an autoencoder**. In this extension, we can just train an autoencoder on the entire dataset (both the labeled and unlabeled set), and then iteratively choose batches which cover the embedding. We won't even have to retrain the autoencoder between batches, since our objective is getting a good cover and not maximizing some individual example score.




TODO: link to next post...


[previous post]: https://dsgissin.github.io/DiscriminativeActiveLearning/2018/07/05/AL-Intro.html
[Yarin Gal Paper]: https://arxiv.org/pdf/1703.02910.pdf
[adversarial example paper]: https://arxiv.org/pdf/1312.6199.pdf
[adversarial AL paper]: https://arxiv.org/pdf/1802.09841.pdf
[Ran Core Set]: https://arxiv.org/pdf/1711.00941.pdf
[Silvio Core Set]: https://arxiv.org/pdf/1708.00489.pdf