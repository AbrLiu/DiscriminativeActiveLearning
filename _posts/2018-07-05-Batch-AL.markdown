---
layout: post
title: Batch Active Learning
---
## Making Active Learning Practical
In our [last post][previous post], we learned about the active learning framework. In our framework, there is a lrage pool of easily gathered unlabeled data, and our learner helps reduce the labeling costs by asking for labels one at a time from the unlabeled pool. We saw a few possible query strategies that the learner can use, and that they can reduce the labeling costs significantly.

This is all very nice, but who has time to sit and label examples one at a time, when we have to wait for the learner to train a neural network before every single label request? Furthermore, we usually want to have a large batch of examples to label, so we can send them to many different annotators who can label them in parallel... So to make the ideas from the last post applicable to modern machine learning tasks, we need to slightly tweak our framework.

In each iteration, the learner will train a model on the labeled examples like before, only now he will **query a batch of examples**. The size of this batch is a free parameter of our framework, and depends on the way we do the labeling. Overall this seems like a rather meaningless change to our framework, but it adds some complexity.

### The Effects of the Batch Size
There are a few effects that the batch size has on our learning process. On the plus side, we get to label examples in parallel and suffer less of a delay to our process which stems from waiting for our neural networks to train. But this plus side has a cost.

On the down side, the learner has to select examples with less information. If a learner has to query a batch of size 100, then the 100th example he has to choose is chosen without seeing the labels to the first 99 examples. If we had those labels, the learner might have chosen a different 100th example which would have been better for our overall accuracy in the long run.

But there is an even worse down side to the batch setting. The learner can ask for a batch of 100 examples such that each of them is very informative, **but the information they give us is too similar**. Intuitively, if the learner has trouble confidently classifying the digit "0", it should be enough to get a few relevant examples of difficult "0"s for it to improve. But if the learner asks for 100 examples of "0"s, it isn't using it's batch budget very well...

This sort of problem can lead to a greedy strategy which chooses the top-K examples according to a standard measure **being worse than random sampling**. So intuitively, a good query strategy for the batch setting should not only chooses informative examples in the batch, but also choose examples that have small mutual information.

## Query Strategies For the Batch Setting
There were many papers published in recent years about active learning for neural networks, and we probably missed a few of them. Still, we'll try to detail the most notable modern methods, and for those we also provide implementations and code to recreate our experiments.

### Greedy Query Strategies
These strategies are adaptations for neural networks of classic query strategies which only query one label at a time. They do not take into account the mutual information between examples in a single batch, but rather simply **choose the top-K examples under the relevant score**.

#### Uncertainty-Based Strategy
We saw in the previous post that uncertainty sampling performs really well in the classical setting, and in general it is the most widely used query strategy due to it's effectiveness and simplicity. It isn't surprising then that this query strategy has been adapted to neural networks.
The most straightforward adaptation of uncertainty sampling is to simply treat the softmax scores of the neural network as a probability distribution over the labels \\(\hat{P}(y|x)\\), and use the regular decision rules. This approach is reasonable, but it is questionable how much the softmax scores can be interpreted as probabilities... They look like probabilities, but the training process of the neural network just tries to get the softmax scores of the training examples to be as close to one-hot as possible, and because the model is very powerful it tends to fit the data very well and so gets over confident about examples in general.

To deal with this issue, many people are working on ways of making neural networks output results which more accurately represent their confidence on a given input. The most well known work, and one which has been adapted to the active learning setting, is [Yarin Gal's work on Bayesian Neural Networks][Yarin Gal Paper]. In this work, the authors show that we can get an approximation of the posterior probability over the labels by running the input many times through the network **with dropout turned on** and then average the resulting softmax scores. This method can be easilly implemented for most modern neural network architectures, since they already use dropout for regularization. The resulting probability distribution over the labels after \\(T\\) iterations is then simply:

$$ P(y|x) \approx \frac{1}{T}\sum_{t=1}^{T}\hat{P}(y|x,drop_t) $$

This new confidence estimation can now be plugged into any of the possible uncertainty decision rules in order to choose the top-K examples. In their paper the authors compare many possible decision rules, and the best two decision rules were the two presented in the previous post for uncertainty sampling.

#### Margin-Based Strategy
In the previous post we saw that for linear models there is an equivalence between choosing examples which have a high uncertainty and choosing examples which are closest to the decision boundary. This is due to the fact that the probability of an example is decided by it's distance (or inner product) from the weight vector which is orthogonal to the decision boundary.

In neural networks though, this is hardly the case. The uncertainty score in neural networks is calculated with respect to the final representation layer. This means that choosing according to the uncertainty score is equivalent to choosing examples whose **representation** is closest to the decision boundary. The thing is, because the network is deep and complex, small changes to the input image can cause wild changes to the representation of the image. This is best seen in [adversarial examples][adversarial example paper].

Adversarial examples are a fascinating and quirky property of neural networks which stems from a very simple fact - in the same way we can calculate the derivative of the loss with respect to the weights of the network, we can calculate the derivative **with respect to the input**. So changing the weights in the opposite direction of the gradient, we can change the input image in the direction of the gradient and get a new image which slightly increases the loss. Doing this a few times can result in an image which is completely misclassified while looking basically the same as the original image. This property of neural networks and it's implications has led to a thriving field of neural network security research.

So coming back to active learning, we would like to be able to find the images which are closest to the decision boundary of our network, but the distance of the representation of the inputs to the softmax decision boundary is a bad proxy for that. The actual distance to the decision boundary, like many things in the world of deep learning, is intractable. So what can we do?

The cool solution [Frédéric Precioso's group came up with][adversarial AL paper], is to use adversarial examples to get a better proxy for the distance to the decision boundary. What they do is take each image in the unlabeled set, get it's classification according to the network and then find an adversarial example for it, such that the network makes the wrong classification on that image. They then calculate the distance between the original image and the adversarial example, and choose the top-K examples with the smallest distance to their adversarial example. Basically, they cross the decision boundary and check how far they had to go to cross it.

This doesn't assure that the adversarial example is the closest possible to the decision boundary, but it does give a reasonable heuristic upper bound on the distance to the decision boundary, which their method attempts to minimize.


TODO: link to next post...


[previous post]: https://dsgissin.github.io/DiscriminativeActiveLearning/2018/07/05/AL-Intro.html
[Yarin Gal Paper]: https://arxiv.org/pdf/1703.02910.pdf
[adversarial example paper]: https://arxiv.org/pdf/1312.6199.pdf
[adversarial AL paper]: https://arxiv.org/pdf/1802.09841.pdf
