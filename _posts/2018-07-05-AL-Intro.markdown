---
layout: post
title:  Introduction to Active Learning
---
## The Basic Setup
The idea of AL is, instead of just giving the learner a lot of data to learn from, to **allow the learner to ask questions** about the given data. In particular, the learner gets to ask an oracle (some human annotator) about the label of certain instances that are currently unlabeled. If the learner asks smart questions, he might be able to get examples that are very informative and reach a high level of generalization accuracy with a much smaller labeled dataset than he would have if that dataset had been created using random sampling.

The classic motivating example would be the following 2-dimensional data, which we are trying to separate using some linear classifier:

{% include image.html path="AL-Intro/blobs.png" %}

Clearly we can separate these two blobs linearly using a vertical decision boundary, but this is not so obvious when we only get to see the labels for a small number of these points. Given the wrong subset of these points, we could choose a decision boundary that fails to separate the data correctly...

Let's compare random sampling to an active learning algorithm called "uncertainty sampling" (which we will explain later). Both algorithms get one initial labeled point from each blob (marked in red) and sequentially ask for the label of an additional point (marked in green). In every iteration, we train a SVM classifier on the labeled set:

{% include image.html path="AL-Intro/blobs-AL.png" %}

The two initial points give us a slightly tilted decision boundary that doesn't separate the data well. We can see that random sampling chooses points in the dense yellow blob, which don't give us much information and so the decision boundary stays more or less the same, and we aren't able to separate the data well. On the other hand, the active learning algorithm is able to choose points which are very informative and quickly gets to the correct decision boundary.

This is the promise of active learning - **reduce the amount of samples needed to get a strong classifier by choosing the right examples to label**. 


## Possible Scenarios

There are three main scenarios where active learning has been studied. In all scenarios, at each iteration a model is fitted to the current labeled set and that model is used to decide which unlabeled example we should label next.

{% include image.html path="AL-Intro/scenarios.png" %}

The first scenario is **membership query synthesis**, where the active learner is expected to produce an example that it would like us to label. This scenario requires that the model will be able to capture the data distribution well enough to create examples which are reasonable and that would have a clear label - if we are classifying images and the learner produces an image that is pure noise, we won't be able to label it... Due to this limitation, people grew less interested in this scenario, although it has been picked up recently [thanks to GANs][gan paper] and their ability to model the given data distribution well.

The next scenario is **stream based**, where the learner gets a stream of examples from the data distribution and decides if a given instance should be labeled or not.

The third scenario, and the one we will focus on here, is **pool based active learning**. In this scenario the learner has access to a large pool of unlabeled examples and chooses an example to be labeled from that pool. This scenario is most relevant for when gathering data is simple (scraping images/text from the web for instance), but the labeling process is expensive.


## Query Strategies
Most active learning algorithms start the same way - we fit a model to the labeled set and so we have access to \\(\hat{P}(y|x)\\). Where they differ is how they choose the example to query, given the trained model - the **query strategy**. There are many possible heuristics for choosing the best exmample to query, and we'll detail the most common methods.

### Uncertainty Sampling
This is probably the simplest and most straightforward idea - make the learner query the example which it is least certain about. For binary classification, this boils down to choosing the example on which the model's prediction is closest to a coin toss. When we are working in a multiclass setting, there are different ways of defining uncertainty. The most popular ways are either **top confidence** or **maximum entropy**.

Top confidence looks only at the label that the model would have predicted for the example, and asks how much probability was given to said label. It then chooses the example where the top label had the smallest probability. This is the simplest extension of the binary classification setting, and leads to the following decision rule:

\\(\underset{x}{\mathrm{argmin}}\underset{y}{\mathrm{max}}(\hat{P}(y|x)))\\)

$$ \underset{x}{\mathrm{argmin}}\underset{y}{\mathrm{max}}(\hat{P}(y|x))) $$

Maximum entropy uses the label distribution's entropy as a measure for the uncertainty of the model on an example. This takes into account the probabilities of all possible labels. The resulting decision rule then becomes: 

\\(\underset{x}{\mathrm{argmin}}-\sum_{y \in Y}\hat{P}(y|x))\\)





[gan paper]: TODO